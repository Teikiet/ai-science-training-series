{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/05_llm_part2/Intro_to_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrpuNlbTlwc2"
   },
   "source": [
    "### Recommended Reading\n",
    "- [Basic Prompt Engineering - Large Language Models (LLMs): Tutorial Workshop (Argonne National Laboratory)](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/02-basic-prompt-engineering/Tutorial_02_Prompt_Engineering.ipynb)\n",
    "- [LLM Prompt Engineering for Beginners: What It Is and How to Get Started](https://medium.com/thedeephub/llm-prompt-engineering-for-beginners-what-it-is-and-how-to-get-started-0c1b483d5d4f)\n",
    "-[Prompt Engineering Guide](https://www.promptingguide.ai/techniques/knowledge)\n",
    "- [Prompt Engineering for LLMs: The Art and Science of Building Large Language Model-Based Applications](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/&ved=2ahUKEwj35OqfkKWJAxVJhIkEHSYNAiwQFnoECCwQAQ&usg=AOvVaw091NCqhKa-GArTW_MOSXJu)\n",
    "-[Prompt Engineering for Arithmetic Reasoning Problems](https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e)\n",
    "-[Prompt engineering: overview and guide](https://cloud.google.com/discover/what-is-prompt-engineering)\n",
    "- [How Is ChatGPT’s Behavior Changing over Time?](https://arxiv.org/pdf/2307.09009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkexlqyIlk0y"
   },
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MekoIU8SlqIY"
   },
   "source": [
    "1. Load in a generative model using the HuggingFace pipeline. Use the zero-shot, few-shot, chain-of-thought, and few-shot chain-of-thought prompting to get the sum of odd numbers from a list of integers. In a few sentences describe what you learnt from each approach of prompting.\n",
    "- Next, play around with the temperature parameter. In a few sentences describe what you changes you notice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter huggingfacehub api token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load remote model\n",
    "from langchain.llms import HuggingFaceHub\n",
    "model = \"meta-llama/Llama-3.2-1B\"\n",
    "#model = \"tiiuae/falcon-40b-instruct\"\n",
    "meta = HuggingFaceHub(\n",
    "    repo_id=model,\n",
    "    model_kwargs={\"temperature\": 0.5,\n",
    "                  \"max_length\": 128},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model = \"meta-llama/Llama-3.2-1B\"\n",
    "#model = \"tiiuae/falcon-40b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "ai_pipeline = transformers.pipeline(\"text-generation\",\n",
    "                                        model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        trust_remote_code=True,\n",
    "                                        device_map=\"auto\"\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_ai(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from llama:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 2000\n",
    "\n",
    "      ai_response = ai_pipeline(prompt,\n",
    "                                      #max_length=500,\n",
    "                                      do_sample=True,\n",
    "                                      top_k=10,\n",
    "                                      num_return_sequences=1,\n",
    "                                      eos_token_id=tokenizer.eos_token_id,\n",
    "                                      **kwargs,\n",
    "                                      )\n",
    "      response = ai_response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = meta.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response\n",
    "\n",
    "def iterative_ai(input, previous_output=\"\", max_length=1000):\n",
    "    \"\"\"\n",
    "    Generates content based on the input prompt, with an option to build on a previous output.\n",
    "\n",
    "    :param input: New prompt to generate content for.\n",
    "    :param previous_output: The accumulated output from previous prompts to build context.\n",
    "    :param max_length: Maximum length of the generation including previous context and new prompt.\n",
    "    :return: The new generated text.\n",
    "    \"\"\"\n",
    "    # Combine the previous output with the new input prompt for context\n",
    "    combined_prompt = f\"{previous_output} {input}\"\n",
    "    print(\"#### Combined Prompt for Iteration:\")\n",
    "    print(combined_prompt)\n",
    "\n",
    "\n",
    "    ai_response = get_completion_ai(combined_prompt,\n",
    "                                            host=\"remote\",\n",
    "                                            )\n",
    "\n",
    "    # Extract the newly generated text, removing the input part to avoid repetition\n",
    "    new_text = ai_response.replace(previous_output, '').strip()\n",
    "    print(\"\\n#### Generated Text:\")\n",
    "    print(new_text)\n",
    "    return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zero Shot Prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6].  \n",
      "\n",
      "#### Response from llama: \n",
      "```python\n",
      "def sum_odd_numbers_in_list(my_list):\n",
      "    \"\"\"Return the sum of the odd numbers in the list.\"\"\"\n",
      "    result = 0\n",
      "    for num in my_list:\n",
      "        if num % 2!= 0:\n",
      "            result += num\n",
      "    return result\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6].  \"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Zero-shot prompting relies on the model's prior knowledge, providing a straightforward solution. In this case, the model directly give a python code to solve this problem. This likely because this kind of question is often be seen in coding. However, this is not what I want it to do. So, I add \"explain in math term\" to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6].  \n",
      "\n",
      "#### Response from llama: \n",
      "\n",
      "The sum of odd numbers in the list [1, 2, 3, 4, 5, 6] is 9. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6].  \"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the model provide the answer directly based on its prior knowledge of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. For example, the sum of odd numbers in [7, 8, 9] is 16. The sum of odd numbers in [1,9,11,20,56] is 21.\n",
      "\n",
      "#### Response from llama: \n",
      "```python\n",
      "def sum_odds(list):\n",
      "    sum = 0\n",
      "    for num in list:\n",
      "        if num % 2!= 0:\n",
      "            sum += num\n",
      "    returnσίας\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. For example, the sum of odd numbers in [7, 8, 9] is 16. The sum of odd numbers in [1,9,11,20,56] is 21.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Few-shot prompting provides context and examples, improving model understanding. In this case the model successfully answer the question correctly, based on the provided examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. First identify the odd numbers in the list, then add them together.\n",
      "\n",
      "#### Response from llama: \n",
      "The sum of the odd numbers in the list [1, 2, 3, 4, 5, 6] is:\n",
      "`sum = 1 + 3 + 5 + 7 = 17`\n",
      "\n",
      "#### User: \n",
      "Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. First identify the odd numbers in the list, then add them together.\n",
      "\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Explain in math term. Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. First identify the odd numbers in the list, then add them together.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Chain-of-thought prompting breaks down complex tasks into simpler steps. However, the model still makes some mistake relate to not knowing the number of element in this list and mistakenly include 7. It also perform a wrong addition operator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Few-Shot Chain-of-Thought Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      " Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. Explain in math term. First identify the odd numbers, then add them together. For example, the sum of odd numbers in [7, 8, 9] is 16. The sum of odd numbers in [10, 16, 25, 7] is 25. The sum of odd numbers in [1,9,11,20,56] is 21.\n",
      "\n",
      "#### Response from llama: \n",
      "\n",
      " securing the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. Explain in math term. First identify the odd numbers, then add them together. For example, the sum of odd numbers in [7, 8, 9] is 16. The sum of odd numbers in [10, 16, 25, 7] is 25. The sum of odd numbers in [1,9,11\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \" Find the sum of odd numbers in the list [1, 2, 3, 4, 5, 6]. Explain in math term. First identify the odd numbers, then add them together. For example, the sum of odd numbers in [7, 8, 9] is 16. The sum of odd numbers in [10, 16, 25, 7] is 25. The sum of odd numbers in [1,9,11,20,56] is 21.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Combining few-shot and chain-of-thought prompting enhances model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#### User: \\nWhat is the capital of France?\\n\\n#### Response from llama: \\nParis, France.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion_ai(\"What is the capital of France?\", host=\"remote\", max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-zkEEeKlmRR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Combined Prompt for Iteration:\n",
      " Write a short story about a mysterious garden.\n",
      "invoking llm at Huggingface Hub\n",
      "\n",
      "#### Generated Text:\n",
      "#### User: \n",
      " Write a short story about a mysterious garden.\n",
      "\n",
      "#### Response from llama:앙코로코\n",
      "I like the idea of a mysterious garden. I think I could write a short story about a mysterious garden. I will start by thinking about the garden. I will imagine the garden to be beautiful and mysterious. The garden will be full of flowers and trees. The garden will be a place where people can go to relax and enjoy nature. I will then think about the characters in the story. I will imagine a man and a woman who are lost in the garden. The\n"
     ]
    }
   ],
   "source": [
    "# Initial Prompt\n",
    "initial_prompt = \"Write a short story about a mysterious garden.\"\n",
    "story_part1 = iterative_ai(initial_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Explain step-by-step how to solve the following algebra problem: If 2x + 3 = 10, what is the value of x?\n",
      "\n",
      "#### Response from llama: \n",
      "The equation is 2x + 3 = 10. Subtract 3 from both sides of the equation to isolate x on one side of the equation: 2x = 7. Divide both sides of the equation by 2 to solve for x: x = 3.5.\n"
     ]
    }
   ],
   "source": [
    "# Prompt for solving an algebra problem with step-by-step explanation - how does the LLM perform?\n",
    "math_problem_prompt = \"Explain step-by-step how to solve the following algebra problem: If 2x + 3 = 10, what is the value of x?\"\n",
    "kwargs = {\"max_length\": 1200}\n",
    "math_problem_solution = get_completion_ai(math_problem_prompt, **kwargs)\n",
    "print(math_problem_solution)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOEwj0AKeV1VsbV3PwMuA5l",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
