{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/argonne-lcf/ai-science-training-series/blob/main/05_llm_part2/Intro_to_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrpuNlbTlwc2"
   },
   "source": [
    "### Recommended Reading\n",
    "- [Basic Prompt Engineering - Large Language Models (LLMs): Tutorial Workshop (Argonne National Laboratory)](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/02-basic-prompt-engineering/Tutorial_02_Prompt_Engineering.ipynb)\n",
    "- [LLM Prompt Engineering for Beginners: What It Is and How to Get Started](https://medium.com/thedeephub/llm-prompt-engineering-for-beginners-what-it-is-and-how-to-get-started-0c1b483d5d4f)\n",
    "-[Prompt Engineering Guide](https://www.promptingguide.ai/techniques/knowledge)\n",
    "- [Prompt Engineering for LLMs: The Art and Science of Building Large Language Model-Based Applications](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/&ved=2ahUKEwj35OqfkKWJAxVJhIkEHSYNAiwQFnoECCwQAQ&usg=AOvVaw091NCqhKa-GArTW_MOSXJu)\n",
    "-[Prompt Engineering for Arithmetic Reasoning Problems](https://towardsdatascience.com/prompt-engineering-for-arithmetic-reasoning-problems-28c8bcd5bf0e)\n",
    "-[Prompt engineering: overview and guide](https://cloud.google.com/discover/what-is-prompt-engineering)\n",
    "- [How Is ChatGPT’s Behavior Changing over Time?](https://arxiv.org/pdf/2307.09009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkexlqyIlk0y"
   },
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MekoIU8SlqIY"
   },
   "source": [
    "1. Load in a generative model using the HuggingFace pipeline. Use the zero-shot, few-shot, chain-of-thought, and few-shot chain-of-thought prompting to get the sum of odd numbers from a list of integers. In a few sentences describe what you learnt from each approach of prompting.\n",
    "- Next, play around with the temperature parameter. In a few sentences describe what you changes you notice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"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\"\n",
    "login(token=hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter huggingfacehub api token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load remote model\n",
    "from langchain.llms import HuggingFaceHub\n",
    "model = \"meta-llama/Llama-3.2-3B\"\n",
    "#model = \"tiiuae/falcon-7b-instruct\"\n",
    "meta = HuggingFaceHub(\n",
    "    repo_id=model,\n",
    "    model_kwargs={\"temperature\": 0.5,\n",
    "                  \"max_length\": 128},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741796128b5546cca86a9ac407b52895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "ai_pipeline = transformers.pipeline(\"text-generation\",\n",
    "                                        model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        trust_remote_code=True,\n",
    "                                        device_map=\"auto\"\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_ai(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from chat:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 3000\n",
    "\n",
    "      ai_response = ai_pipeline(prompt,\n",
    "                                      #max_length=500,\n",
    "                                      do_sample=True,\n",
    "                                      top_k=10,\n",
    "                                      num_return_sequences=1,\n",
    "                                      eos_token_id=tokenizer.eos_token_id,\n",
    "                                      **kwargs,\n",
    "                                      )\n",
    "      response = ai_response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = meta.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response\n",
    "\n",
    "def iterative_ai(input, previous_output=\"\", max_length=1000):\n",
    "    \"\"\"\n",
    "    Generates content based on the input prompt, with an option to build on a previous output.\n",
    "\n",
    "    :param input: New prompt to generate content for.\n",
    "    :param previous_output: The accumulated output from previous prompts to build context.\n",
    "    :param max_length: Maximum length of the generation including previous context and new prompt.\n",
    "    :return: The new generated text.\n",
    "    \"\"\"\n",
    "    # Combine the previous output with the new input prompt for context\n",
    "    combined_prompt = f\"{previous_output} {input}\"\n",
    "    print(\"#### Combined Prompt for Iteration:\")\n",
    "    print(combined_prompt)\n",
    "\n",
    "\n",
    "    ai_response = get_completion_ai(combined_prompt,\n",
    "                                            host=\"remote\",\n",
    "                                            )\n",
    "\n",
    "    # Extract the newly generated text, removing the input part to avoid repetition\n",
    "    new_text = ai_response.replace(previous_output, '').strip()\n",
    "    print(\"\\n#### Generated Text:\")\n",
    "    print(new_text)\n",
    "    return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zero Shot Prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in this list [1,2,3,4,5,6]\n",
      "\n",
      "#### Response from chat: \n",
      "sum = 0\n",
      "for i in range(1,7):\n",
      "    if i%2!= 0:\n",
      "        sum += i\n",
      "print(sum)\n",
      "\n",
      "#### Response from chat: \n",
      "sum = 0\n",
      "for i in range(1,7):\n",
      "    if i%2!= 0:\n",
      "        sum += i\n",
      "print(sum)\n",
      "\n",
      "#### Response from chat: \n",
      "sum = 0\n",
      "for i in range(1,7):\n",
      "    if i%2!= 0:\n",
      "        sum\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in this list [1,2,3,4,5,6]\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Zero-shot prompting relies on the model's prior knowledge, providing a straightforward solution. However, the model just straight up giving out the python code solution for this. This might be bacause there are a lot of coding problem that requires finding sum of odd number. I can try to ask it to answer in math term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in this list [1,2,3,4,5,6]. Explain in mathematical terms.\n",
      "\n",
      "#### Response from chat: \n",
      "[1, 3, 5]\n",
      "\n",
      "#### Response from chat: \n",
      "1 + 3 + 5 = 9\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of odd numbers is 9\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of odd numbers is 9\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of odd numbers is 9\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of odd numbers is 9\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of odd numbers is \n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in this list [1,2,3,4,5,6]. Explain in mathematical terms.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now can answer it correctly and give out the answer 9. Now, I will try to add example to the model to see if it really understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in this list [1,2,3,4,5,6]. For example, the sum of odd numbers in [7, 8, 9, 10, 11, 12] is 27. Explain in mathematical terms.\n",
      "\n",
      "#### Response from chat: \n",
      "Yes, the sum of odd numbers in [1,2,3,4,5,6] is 21. This can be expressed as:\n",
      "\n",
      "sum of odd numbers in [1,2,3,4,5,6] = 1 + 3 + 5 + 7 + 9 + 11 = 21\n",
      "\n",
      "This can be expressed in mathematical terms as:\n",
      "\n",
      "sum of odd numbers in [1,2,3,4,5,6] =\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in this list [1,2,3,4,5,6]. For example, the sum of odd numbers in [7, 8, 9, 10, 11, 12] is 27. Explain in mathematical terms.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Few-shot prompting provides context and examples, improving model understanding. However, in this case the model still give out an incorrect answer for the problem. The model fail to understand what number to add in the list and produce a wrong addition operation. Maybe the model need more information of how to perform the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in this list [1,2,3,4,5,6]. First, you have to list out all odd numbers in [1,2,3,4,5,6]. Then, you have to add those odd number up to get the sum. Explain in mathematical terms.\n",
      "\n",
      "#### Response from chat: \n",
      "You can use the following strategy to find the sum of all odd numbers in a list of numbers: \n",
      "1. Create a new list that only contains the odd numbers from the original list. \n",
      "2. Iterate through the new list and add each odd number to a running total. \n",
      "3. Return the final running total. \n",
      "\n",
      "This strategy involves creating a new list and iterating through it to find the sum of all odd numbers. The code to implement this strategy is as follows: \n",
      "\n",
      "```python\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in this list [1,2,3,4,5,6]. First, you have to list out all odd numbers in [1,2,3,4,5,6]. Then, you have to add those odd number up to get the sum. Explain in mathematical terms.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Chain-of-thought prompting breaks down complex tasks into simpler steps. The model follow instruction and produce lines of steps to solve the problem. But due to the text limit, the model did not complete the answer. Maybe asking the model to answer it shorter would improve the outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Find the sum of odd numbers in this list [1,2,3,4,5,6]. First, you have to list out all odd numbers in [1,2,3,4,5,6]. Then, you have to add those odd number up to get the sum. Explain shortly in mathematical terms.\n",
      "\n",
      "#### Response from chat: \n",
      "We can list out the odd numbers in the list [1,2,3,4,5,6] by using the following formula: \n",
      "``` \n",
      "odd_numbers = [num for num in range(1,7) if num % 2!= 0]\n",
      "```\n",
      "Then, we can add all the odd numbers in the list by using the following formula: \n",
      "``` \n",
      "sum_odd_numbers = sum(odd_numbers)\n",
      "```\n",
      "So, the sum of odd numbers in the list\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Find the sum of odd numbers in this list [1,2,3,4,5,6]. First, you have to list out all odd numbers in [1,2,3,4,5,6]. Then, you have to add those odd number up to get the sum. Explain shortly in mathematical terms.\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model somehow still using the python code to solve the problem, I think providing an example would clear out the confusion for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Few-Shot Chain-of-Thought Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of all odd numbers in this list [1, 2, 3, 4, 5, 6] is 1+3+5+7=16.\n",
      "\n",
      "#### User: \n",
      "I have a question about the problem: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\"\n",
    "response = get_completion_ai(input_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning: Combining few-shot and chain-of-thought prompting enhances model performance. However, in this case, the model still mistakenly give a wrong answer. The model indentfy the odd number and perform the right addition operation, but the model include 7 in the calculation, which is not in the list. Maybe longer input might distract the model from the original question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Changing tempreture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load remote model\n",
    "from langchain.llms import HuggingFaceHub\n",
    "model = \"meta-llama/Llama-3.2-3B\"\n",
    "#model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "def get_completion_ai(input, temp, host=\"remote\", **kwargs):\n",
    "    meta = HuggingFaceHub(\n",
    "    repo_id=model,\n",
    "    model_kwargs={\"temperature\": temp,\n",
    "                  \"max_length\": 128},)\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from chat:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 3000\n",
    "\n",
    "      ai_response = ai_pipeline(prompt,\n",
    "                                      #max_length=500,\n",
    "                                      do_sample=True,\n",
    "                                      top_k=10,\n",
    "                                      num_return_sequences=1,\n",
    "                                      eos_token_id=tokenizer.eos_token_id,\n",
    "                                      **kwargs,\n",
    "                                      )\n",
    "      response = ai_response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = meta.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of all odd numbers in this list [1, 2, 3, 4, 5, 6] is 1 + 3 + 5 = 9. To find the sum, we first list out all odd numbers in the list. Then, we add up all the odd numbers. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd numbers are \n"
     ]
    }
   ],
   "source": [
    "print(get_completion_ai(input_prompt, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.1 tempreture, the model answer the question directly without any deviation from the answer. Out of all the tempreture value that I tested, this one gave the most accuarate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "The sum of all odd numbers in this list [1, 2, 3, 4, 5, 6] is 1 + 3 + 5 + 7 = 16.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion_ai(input_prompt, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.2, the model missunderstand the question and started deviate from the right answer. The model may experience \"halluciation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "The first step is to list out all odd numbers in the list. 1, 3, 5, 7, 9. Add up those odd numbers to get the sum. 1 + 3 + 5 + 7 + 9 = 25. So, the sum of odd numbers in [1, 2, 3, 4, 5, 6] is 25.\n",
      "\n",
      "#### User: \n",
      "Solve and explain this problem step by\n"
     ]
    }
   ],
   "source": [
    "print(get_completion_ai(input_prompt, 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.6, the model halluciate even more and adding more odd numbers that are out of the list to the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "There are many ways to solve this problem. One way is to use a for loop to iterate over the list and check each element if it is odd. If it is, add it to a variable that stores the sum. Then, return the sum. Another way is to use a list comprehension to create a list of only the odd numbers from the list, and then use a for loop to iterate over the list and add up the elements.\n",
      "\n",
      "#### Solution:\n",
      "def get_odd_sum(my_list):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_completion_ai(input_prompt, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.7, the model stopped respond with the answer to problem, but started to explain how to solve for it. This is a lot of deviation from the origional question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "Solve and explain this problem step by step: Find the sum of all odd numbers in this list [1, 2, 3, 4, 5, 6]. First, you have to list out all odd numbers in the list. Then, you have to add those odd number up to get the sum. For example, for the sum of odd numbers in [7, 8, 9, 10, 11, 12], the odd number are 7, 9, 11. The sum of these odd numbers is 27. So, what is the sum of odd numbers in [1, 2, 3, 4, 5, 6]?\n",
      "\n",
      "#### Response from chat: \n",
      "Compute the sum of all odd numbers in the given list [1,2,3,4,5,6]\n",
      "Solution:\n",
      "We can compute the sum of all odd numbers in the given list [1,2,3,4,5,6] as follows:\n",
      "Let us list out all odd numbers in the given list [1,2,3,4,5,6] as follows: 1,3,5,7\n",
      "Now, we compute the sum of these\n"
     ]
    }
   ],
   "source": [
    "print(get_completion_ai(input_prompt, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model at 0.9 gave out longer answer but not going traight to the main point of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, increasing the tempreture of the model makes the answer become more chaos, and the model more likely to produce \"halluciating\" results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. RAG Tutrial Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teikiet/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.quantamagazine.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/home/teikiet/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader([\"https://www.quantamagazine.org/in-the-quantum-world-even-points-of-view-are-uncertain-20241122/\", \"https://www.google.com\"])\n",
    "\n",
    "#To bypass SSL verification errors during fetching, you can set the “verify” option:\n",
    "loader.requests_kwargs = {'verify':False}\n",
    "data = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "#documents = text_splitter.create_documents(documents)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings':False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name = modelPath,\n",
    "  model_kwargs = model_kwargs,\n",
    "  encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renner thinks quantum reference frames may also be central to elucidating the foundations of quantum physics. A few years ago, he and his colleague Daniela Frauchiger designed a quantum thought experiment that creates a logical contradiction. The resulting paradox seems to imply that physicists must give up at least one of many well-accepted notions about our world — say, that quantum theory is universal and applies to human beings as well as atoms.\n",
      "Renner, however, now suspects that the paradox arises simply because physicists haven’t carefully accounted for reference frames. No one has yet figured out how to rewrite this or other thought experiments using quantum reference frames, but doing so “is very likely to lead us to the solution of the paradoxes,” he said.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "question = \"Who is Renato Renner?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from zmq import device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "llm = HuggingFacePipeline(\n",
    "   pipeline = pipe,\n",
    "   model_kwargs={\"temperature\": 0, \"max_length\": 2048},\n",
    ")\n",
    "#'HuggingFaceH4/zephyr-7b-beta'\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teikiet/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a physicist at the University of Vienna and director of the Institute for Quant\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "  llm=llm,\n",
    "  chain_type=\"stuff\",\n",
    "  retriever=db.as_retriever(),\n",
    "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "result = qa_chain ({ \"query\" : \"Who is Brukner??\" })\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOEwj0AKeV1VsbV3PwMuA5l",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
