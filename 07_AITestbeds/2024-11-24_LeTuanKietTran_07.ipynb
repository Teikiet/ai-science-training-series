{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions and Answers\n",
    "\n",
    "## 1. What are the key architectural features that make these systems suitable for AI workloads?  \n",
    "AI accelerators like SambaNova, Cerebras, Graphcore, and Groq are tailored for AI tasks due to their specialized hardware optimized for matrix multiplications and tensor operations. They feature high memory bandwidth and substantial on-chip memory to handle memory-intensive workloads. Additionally, their architectures emphasize scalability and parallelism, enabling efficient data processing across multiple cores, which significantly accelerates training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.  \n",
    "- **SambaNova**: Its Reconfigurable Dataflow Unit (RDU) enables flexible dataflow processing, supported by a multi-tiered memory design with terabytes of addressable memory, ideal for large datasets.  \n",
    "- **Cerebras**: The Wafer-Scale Engine (WSE) uses independent processing elements (PEs) with dedicated memory and fine-grained dataflow control, enabling high parallelism and scalability.  \n",
    "- **Graphcore**: The Intelligence Processing Unit (IPU) integrates many interconnected processing tiles, each with local memory. It employs Bulk Synchronous Parallelism (BSP), alternating between computation and communication phases.  \n",
    "- **Groq**: The Tensor Streaming Processor (TSP) focuses on deterministic execution, which is particularly beneficial for low-latency inference tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?  \n",
    "\n",
    "Refactoring an AI model to run on ALCF’s AI testbeds, such as SambaNova or Cerebras, involves adapting the model to meet the unique architectural and software requirements of these platforms. The process begins with understanding the target platform, as SambaNova relies on a dataflow-based architecture with Reconfigurable Dataflow Units, while Cerebras uses a wafer-scale engine optimized for dense tensor computations. The next step is preparing the model, typically built in PyTorch or TensorFlow, and setting up the corresponding SDKs like SambaFlow or the Cerebras Software Stack (CSS). Refactoring involves aligning the model’s structure with the platform’s operational paradigm—optimizing dataflow for SambaNova or configuring large-scale tensor operations for Cerebras. The model is then compiled using platform-specific tools, such as the Cerebras compiler or SambaFlow APIs, to ensure compatibility and efficiency. Post-deployment, iterative performance tuning and profiling are conducted to optimize execution, followed by verifying accuracy and scientific validity. Tools like profiling utilities, batch size configurators, and tensor placement strategies assist in this process.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Give an example of a project that would benefit from AI accelerators and why.  \n",
    "Deep learning-based cancer diagnosis is a prime example. Processing medical images like MRIs and CT scans for early cancer detection requires intensive computations. AI accelerators expedite training and inference while enabling real-time diagnostics and reducing power consumption. Benefits include quicker and more accurate diagnoses, improved research capabilities, and cost savings. Suitable accelerators for such tasks include SambaNova's RDU, Cerebras' WSE, Graphcore's IPU, and Groq's TSP, all of which contribute to better patient outcomes and advancements in medical research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
